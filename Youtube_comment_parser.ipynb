{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Youtube_comment_parser.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahmedshahriar/youtube-comment-scraper/blob/main/Youtube_comment_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74Cn074TFXtB"
      },
      "source": [
        "# Download Comments from Youtube videos\n",
        "\n",
        "GitHub Repo : [youtube-comment-scraper](https://github.com/ahmedshahriar/youtube-comment-scraper)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ijjbm-fAvC3"
      },
      "source": [
        "# !pip install lxml\n",
        "# The script is based on https://github.com/egbertbouman/youtube-comment-downloader\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "\n",
        "import lxml.html\n",
        "import requests\n",
        "\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "YOUTUBE_COMMENTS_AJAX_URL = 'https://www.youtube.com/comment_service_ajax'\n",
        "\n",
        "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
        "\n",
        "SORT_BY_POPULAR = 0\n",
        "SORT_BY_RECENT = 1\n",
        "\n",
        "YT_CFG_RE = r'ytcfg\\.set\\s*\\(\\s*({.+?})\\s*\\)\\s*;'\n",
        "YT_INITIAL_DATA_RE = r'(?:window\\s*\\[\\s*[\"\\']ytInitialData[\"\\']\\s*\\]|ytInitialData)\\s*=\\s*({.+?})\\s*;\\s*(?:var\\s+meta|</script|\\n)'\n",
        "\n",
        "FILE_NAME = 'ytb_comments.csv'\n",
        "\n",
        "def regex_search(text, pattern, group=1, default=None):\n",
        "    match = re.search(pattern, text)\n",
        "    return match.group(group) if match else default\n",
        "\n",
        "\n",
        "def ajax_request(session, endpoint, ytcfg, retries=5, sleep=20):\n",
        "    url = 'https://www.youtube.com' + endpoint['commandMetadata']['webCommandMetadata']['apiUrl']\n",
        "    \n",
        "    data = {'context': ytcfg['INNERTUBE_CONTEXT'],\n",
        "            'continuation': endpoint['continuationCommand']['token']}\n",
        "\n",
        "    for _ in range(retries):\n",
        "        response = session.post(url, params={'key': ytcfg['INNERTUBE_API_KEY']}, json=data)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        if response.status_code in [403, 413]:\n",
        "            return {}\n",
        "        else:\n",
        "            time.sleep(sleep)\n",
        "\n",
        "\n",
        "def download_comments(YOUTUBE_VIDEO_URL, sort_by=SORT_BY_RECENT, language=None, sleep=.1):\n",
        "    session = requests.Session()\n",
        "    session.headers['User-Agent'] = USER_AGENT\n",
        "\n",
        "    # response = session.get(YOUTUBE_VIDEO_URL.format(youtube_id=youtube_id))\n",
        "    response = session.get(YOUTUBE_VIDEO_URL)\n",
        "\n",
        "    if 'uxe=' in response.request.url:\n",
        "        session.cookies.set('CONSENT', 'YES+cb', domain='.youtube.com')\n",
        "        response = session.get(YOUTUBE_VIDEO_URL.format(youtube_id=youtube_id))\n",
        "\n",
        "    html = response.text\n",
        "    ytcfg = json.loads(regex_search(html, YT_CFG_RE, default=''))\n",
        "    if not ytcfg:\n",
        "        return # Unable to extract configuration\n",
        "    if language:\n",
        "        ytcfg['INNERTUBE_CONTEXT']['client']['hl'] = language\n",
        "\n",
        "    data = json.loads(regex_search(html, YT_INITIAL_DATA_RE, default=''))\n",
        "\n",
        "    section = next(search_dict(data, 'itemSectionRenderer'), None)\n",
        "    renderer = next(search_dict(section, 'continuationItemRenderer'), None) if section else None\n",
        "    if not renderer:\n",
        "        # Comments disabled?\n",
        "        return\n",
        "\n",
        "    needs_sorting = sort_by != SORT_BY_POPULAR\n",
        "    continuations = [renderer['continuationEndpoint']]\n",
        "    while continuations:\n",
        "        continuation = continuations.pop()\n",
        "        response = ajax_request(session, continuation, ytcfg)\n",
        "\n",
        "        if not response:\n",
        "            break\n",
        "        if list(search_dict(response, 'externalErrorMessage')):\n",
        "            raise RuntimeError('Error returned from server: ' + next(search_dict(response, 'externalErrorMessage')))\n",
        "\n",
        "        if needs_sorting:\n",
        "            sort_menu = next(search_dict(response, 'sortFilterSubMenuRenderer'), {}).get('subMenuItems', [])\n",
        "            if sort_by < len(sort_menu):\n",
        "                continuations = [sort_menu[sort_by]['serviceEndpoint']]\n",
        "                needs_sorting = False\n",
        "                continue\n",
        "            raise RuntimeError('Failed to set sorting')\n",
        "\n",
        "        actions = list(search_dict(response, 'reloadContinuationItemsCommand')) + \\\n",
        "                  list(search_dict(response, 'appendContinuationItemsAction'))\n",
        "        for action in actions:\n",
        "            for item in action.get('continuationItems', []):\n",
        "                if action['targetId'] == 'comments-section':\n",
        "                    # Process continuations for comments and replies.\n",
        "                    continuations[:0] = [ep for ep in search_dict(item, 'continuationEndpoint')]\n",
        "                if action['targetId'].startswith('comment-replies-item') and 'continuationItemRenderer' in item:\n",
        "                    # Process the 'Show more replies' button\n",
        "                    continuations.append(next(search_dict(item, 'buttonRenderer'))['command'])\n",
        "\n",
        "        for comment in reversed(list(search_dict(response, 'commentRenderer'))):\n",
        "            yield {'cid': comment['commentId'],\n",
        "                   'text': ''.join([c['text'] for c in comment['contentText'].get('runs', [])]),\n",
        "                   'time': comment['publishedTimeText']['runs'][0]['text'],\n",
        "                   'author': comment.get('authorText', {}).get('simpleText', ''),\n",
        "                   'channel': comment['authorEndpoint']['browseEndpoint'].get('browseId', ''),\n",
        "                   'votes': comment.get('voteCount', {}).get('simpleText', '0'),\n",
        "                   'photo': comment['authorThumbnail']['thumbnails'][-1]['url'],\n",
        "                   'heart': next(search_dict(comment, 'isHearted'), False)}\n",
        "\n",
        "        time.sleep(sleep)\n",
        "\n",
        "\n",
        "def search_dict(partial, search_key):\n",
        "    stack = [partial]\n",
        "    while stack:\n",
        "        current_item = stack.pop()\n",
        "        if isinstance(current_item, dict):\n",
        "            for key, value in current_item.items():\n",
        "                if key == search_key:\n",
        "                    yield value\n",
        "                else:\n",
        "                    stack.append(value)\n",
        "        elif isinstance(current_item, list):\n",
        "            for value in current_item:\n",
        "                stack.append(value)\n",
        "\n",
        "\n",
        "def main(url):\n",
        "    df_comment = pd.DataFrame()\n",
        "    try:\n",
        "        youtube_url = url\n",
        "        limit = 100\n",
        "\n",
        "        print('Downloading Youtube comments for video:', youtube_url)\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for comment in download_comments(youtube_url):\n",
        "\n",
        "            df_comment = df_comment.append(comment, ignore_index=True)\n",
        "\n",
        "            # comments overview\n",
        "            comment_json = json.dumps(comment, ensure_ascii=False)\n",
        "            print(comment_json)\n",
        "\n",
        "            count += 1\n",
        "\n",
        "            if limit and count >= limit:\n",
        "                break\n",
        "\n",
        "        print(df_comment.shape, df_comment)\n",
        "\n",
        "        if not os.path.isfile(FILE_NAME):\n",
        "            df_comment.to_csv(FILE_NAME, encoding='utf-8', index=False)\n",
        "        else:  # else it exists so append without writing the header\n",
        "            df_comment.to_csv(FILE_NAME, mode='a', encoding='utf-8', index=False, header=False)\n",
        "\n",
        "        print('\\n[{:.2f} seconds] Done!'.format(time.time() - start_time))\n",
        "\n",
        "    except Exception as e:\n",
        "        print('Error:', str(e))\n",
        "        sys.exit(1)                            \n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Dump to a csv  from a single video\"\"\"\n",
        "# youtube_URL = 'https://www.youtube.com/watch?v=Ucbrmw2qtXs'\n",
        "# main(youtube_URL)\n",
        "\n",
        "\"\"\"\n",
        "Dump to a csv from a a csv with video links\n",
        "NB create a csv with one column titled 'link'\n",
        "a sample is given below\n",
        "\n",
        "'ytb_video_list.csv'\n",
        "\n",
        "link\n",
        "https://www.youtube.com/watch?v=-t_uhBBDbA4\n",
        "https://www.youtube.com/watch?v=75vjjRza7IU\n",
        "https://www.youtube.com/watch?v=j6dmaPzOBHY\n",
        "https://www.youtube.com/watch?v=Yj2efyQV1RI\n",
        "https://www.youtube.com/watch?v=HV652F7U6Qs\n",
        "https://www.youtube.com/watch?v=47iXEucg3eo\n",
        "https://www.youtube.com/watch?v=ofHXBLEE3TQ\n",
        "https://www.youtube.com/watch?v=X6lGqSfVRT8\n",
        "https://www.youtube.com/watch?v=a_-z9FhGBrE\n",
        "https://www.youtube.com/watch?v=wTUM_4cVlE4\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# df_video_list = pd.read_csv('ytb_video_list.csv')\n",
        "# print(df_video_list['link'].map(lambda x: main(x)))\n",
        "# print(main(pd.read_csv('ytb_video_list.csv')['link']))\n",
        "\n",
        "\n",
        "\"\"\"Dump to a csv from a a list with video links\"\"\"\n",
        "ytb_video_list = ['https://www.youtube.com/watch?v=-t_uhBBDbA4',\n",
        "                  'https://www.youtube.com/watch?v=75vjjRza7IU',\n",
        "                  'https://www.youtube.com/watch?v=j6dmaPzOBHY']\n",
        "\n",
        "for video_link in ytb_video_list:\n",
        "    main(video_link)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQpFQMAxSxzS"
      },
      "source": [
        "pd.read_csv('/content/ytb_comments.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}